{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, BaseCrossValidator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "#Laden der Daten und Vorbereiten für das maschinelle Lernen:\n",
    "DATA_STORE = 'sp.h5'\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    data = store.get('data_clean')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        row.prices.date\n",
    "    except:\n",
    "        row.prices = row.prices.reset_index(level=['date'])\n",
    "\n",
    "data.rename_axis('ticker').reset_index()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    row.prices['ticker'] = index\n",
    "\n",
    "data_flat = pd.DataFrame(columns=['ticker', 'date', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'weekly_return', 'rsi', 'bb_low', 'bb_mid', 'bb_upper', 'target'])\n",
    "data_flat.set_index('ticker')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    data_flat = pd.concat([data_flat, row.prices])\n",
    "\n",
    "# Berechnung der wöchentlichen Renditen und Klassifizierung der Performance:\n",
    "data_flat['weekly_return'] = data_flat['adjusted_close'].pct_change(1).shift(-1)\n",
    "\n",
    "outperform_threshold = 0.015\n",
    "underperform_threshold = -0.01\n",
    "\n",
    "def classify_performance(weekly_return):\n",
    "    if weekly_return > outperform_threshold:\n",
    "        return 1\n",
    "    elif weekly_return < underperform_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data_flat['target'] = data_flat['weekly_return'].apply(classify_performance)\n",
    "data_flat = data_flat.dropna()\n",
    "\n",
    "data_flat['date'] = pd.to_datetime(data_flat['date']) #Alternativ\n",
    "data_flat = data_flat[data_flat['date'].dt.weekday == 4]\n",
    "\n",
    "#data_flat = data_flat.drop(545) # weekly return von 400% bei CPWR 27.1.2017\n",
    "\n",
    "#CPWR komplett entfernen\n",
    "data_flat = data_flat[data_flat['ticker'] != 'CPWR']\n",
    "\n",
    "print(data_flat.index.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    print(store.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative Walk Forward Testen:\n",
    "#Implementierung der Walk-Forward-Cross-Validation:\n",
    "class WalkForwardCV(BaseCrossValidator):\n",
    "    def __init__(self, n_splits=4, test_period_length=8, lookahead=1):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_period_length = test_period_length\n",
    "        self.lookahead = lookahead\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        for i in range(self.n_splits):\n",
    "            train_start = 0\n",
    "            train_end = i * self.test_period_length + (n_samples - self.test_period_length * (self.n_splits + self.lookahead - 1))\n",
    "            test_start = train_end\n",
    "            test_end = test_start + self.test_period_length\n",
    "            yield np.arange(train_start, train_end), np.arange(test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    monthly_constituents = store.get('monatliche_bestandteile')\n",
    "\n",
    "#Hinzufügen insp500 Der RF kann auf daten trainieren, die nicht im sp500 waren, aber anschließend nur in die zu dem Zeitpunkt enthaltenen Werte investieren\n",
    "def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "    tickers_on_date = set()\n",
    "    for idx, row in monthly_constituents.iterrows():\n",
    "        if row['Date'] <= date:\n",
    "            tickers_on_date = row['Constituents']\n",
    "            break\n",
    "    return tickers_on_date\n",
    "\n",
    "\n",
    "#data_flat['in_sp500'] = [ticker in get_sp500_tickers_on_date(date, monthly_constituents) for date, ticker in zip(data_flat['date'], data_flat['ticker'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung der Daten in Trainings-, Validierungs- und Testsets\n",
    "features = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'rsi', 'bb_low', 'bb_mid', 'bb_upper']\n",
    "target = 'target'\n",
    "\n",
    "# Anpassung der Zeitraum für Trainings- und Testphasen\n",
    "n_splits = 10    #10 #4\n",
    "train_period_length = 2  # 4, 8, 16 Wochen Trainingszeitraum auf längeren trainingszeitraum \n",
    "test_period_length = 3    # 4,8, 16 Wochen Testzeitraum\n",
    "lookahead = 1\n",
    "\n",
    "cv = WalkForwardCV(n_splits=n_splits, test_period_length=test_period_length, lookahead=lookahead)\n",
    "\n",
    "#Finanzkrise mit betrachten; Zeitraum ab 2010 definieren, da sich vielleicht Muster geändert haben\n",
    "\n",
    "\n",
    "train_start_date = '2009-12-31'\n",
    "train_end_date = '2010-12-31'\n",
    "valid_end_date = '2011-12-31'\n",
    "test_end_date = '2022-12-31'\n",
    "\n",
    "test_start_date = '2012-01-01'\n",
    "\n",
    "train_start_date = pd.Timestamp(train_start_date)\n",
    "train_end_date = pd.Timestamp(train_end_date)\n",
    "valid_end_date = pd.Timestamp(valid_end_date)\n",
    "test_end_date = pd.Timestamp(test_end_date)\n",
    "\n",
    "test_start_date = pd.Timestamp(test_start_date)\n",
    "\n",
    "\n",
    "train_data = data_flat[data_flat['date'] <= train_end_date]\n",
    "valid_data = data_flat[(data_flat['date'] > train_end_date) & (data_flat['date'] <= valid_end_date)]\n",
    "test_data = data_flat[(data_flat['date'] > valid_end_date) & (data_flat['date'] >= test_start_date) & (data_flat['date'] <= test_end_date)]\n",
    "\n",
    "X_train = train_data[features]\n",
    "\n",
    "\n",
    "y_train = train_data[target]\n",
    "X_valid = valid_data[features]\n",
    "\n",
    "\n",
    "y_valid = valid_data[target]\n",
    "X_test = test_data[features]\n",
    "\n",
    "y_test = test_data[target]\n",
    "\n",
    "# param_grid = {\n",
    "#     'rf__class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "#     # Andere zu testende Parameter hier\n",
    "# }\n",
    "\n",
    "# Erweiterte Hyperparameter-Optimierung\n",
    "class_weights = {-1: 1, 0: 1, 1: 3}  # Klasse 1 wird dreifach gewichtet Alternativ: 2fach getestet\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='mean')),    \n",
    "    #('scaler', StandardScaler()),    \n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight=class_weights))\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50],#[10,50,60,100]#, 50], #100, 200],#, 100], #,100, 200, 500],\n",
    "    'rf__max_depth': [30],#[None, 10, 20,30,40],#, 30],#, 40],# 20], #, 30, 40],\n",
    "    'rf__min_samples_split': [2], #[2, 5, 10], 10],#, 20],# 10],#, 20],\n",
    "    'rf__min_samples_leaf': [2],#[1, 2, 4, 6], 4, 8],# 4], # 8],\n",
    "    'rf__max_features': [None]#['sqrt', 'log2', None] #Die Verwendung von log2 als Option für rf__max_features ermöglicht es, verschiedene Arten von Entscheidungsbäumen innerhalb des Random Forest-Modells zu erstellen. Durch das Berücksichtigen einer kleineren Anzahl von Merkmalen bei jedem Split können Sie ein Modell erhalten, das weniger anfällig für Overfitting ist und eine bessere Generalisierung auf neue Daten bietet.\n",
    "}\n",
    "\n",
    "# Verwendung von GridSearchCV für Hyperparameter-Optimierung mit Genauigkeit als Scoring-Methode\n",
    "grid = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Alternative Scoring-Methode: F1-Score (Macro) statt nur Genauigkeit\n",
    "grid_f1 = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Messung der Laufzeit der Hyperparameter-Optimierung\n",
    "start_time = time.time()\n",
    "grid.fit(X_train, y_train)\n",
    "grid_f1.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Laufzeit der Hyperparameter-Optimierung: {:.2f} Minuten\".format((end_time - start_time) / 60))\n",
    "\n",
    "# Ergebnisse der Walk-Forward-Cross-Validation\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "#Ergebnisse für jeden Split anzeigen\n",
    "for i in range(n_splits):\n",
    "    split_test_score = f'split{i}_test_score'\n",
    "    print(f\"Testergebnisse für Split {i + 1}:\")\n",
    "    print(cv_results[[split_test_score]].sort_values(by=split_test_score, ascending=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Ausgabe der besten Hyperparameter und Modell-Performance\n",
    "print(\"Beste Hyperparameter: \", grid.best_params_)\n",
    "print(\"Beste Modellgenauigkeit: {:.4f}\".format(grid.best_score_))\n",
    "print(\"Beste F1-Score (Macro) (Alternative): {:.4f}\".format(grid_f1.best_score_))\n",
    "\n",
    "# Testen des besten Modells auf den Validierungsdaten\n",
    "best_model = grid_f1.best_estimator_\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# Drucken der Leistungsmetriken\n",
    "print(\"Genauigkeit: \", accuracy_score(y_valid, y_valid_pred))\n",
    "print(\"Klassifikationsbericht: \")\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Testen des besten Modells auf den Testdaten\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Leistungsmetriken für das Testset\n",
    "print(\"Test-Genauigkeit: \", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test-Klassifikationsbericht: \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Feature-Wichtigkeit\n",
    "feature_importance = best_model.named_steps['rf'].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(X_train.shape[1]), feature_importance[sorted_idx])\n",
    "plt.yticks(range(X_train.shape[1]), X_train.columns[sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance für das beste Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung der wöchentlichen Renditen und Vorhersagen\n",
    "test_data['predicted'] = y_test_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(test_data['date'], test_data['weekly_return'], label='True Returns')\n",
    "ax.scatter(test_data['date'][test_data['predicted'] == 1], test_data['weekly_return'][test_data['predicted'] == 1], color='g', label='Outperform Prediction')\n",
    "ax.scatter(test_data['date'][test_data['predicted'] == -1], test_data['weekly_return'][test_data['predicted'] == -1], color='r', label='Underperform Prediction')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Weekly Return')\n",
    "ax.legend()\n",
    "plt.title('Weekly Returns und Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backtest auf Validierungs- und Testset\n",
    "\n",
    "validation_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "print(\"Validierungsgenauigkeit: {:.2f}%\".format(validation_accuracy * 100))\n",
    "\n",
    "print(\"\\nKlassifikationsbericht für Validierung:\")\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Erstellen von Maske, um den unbekannten Zeitraum im Testset zu extrahieren; eigtl. unnötig da testzeitraum definiert\n",
    "mask = test_data['date'] > valid_end_date # + pd.DateOffset(weeks=1)\n",
    "unknown_test_data = test_data[mask]\n",
    "unknown_X_test = unknown_test_data[features]\n",
    "unknown_y_test = unknown_test_data[target]\n",
    "\n",
    "# Vorhersagen auf dem unbekannten Zeitraum im Testset\n",
    "y_test_pred = grid.predict(unknown_X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(unknown_y_test, y_test_pred)\n",
    "print(\"Testgenauigkeit: {:.2f}%\".format(test_accuracy * 100))\n",
    "\n",
    "print(\"\\nKlassifikationsbericht für Test:\")\n",
    "print(classification_report(unknown_y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Der beste RandomForestClassifier, der während der Hyperparameter-Optimierung gefunden wurde\n",
    "best_rf = grid.best_estimator_.named_steps['rf']\n",
    "\n",
    "# Berechnung der Feature Importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "\n",
    "# Erstellen eines DataFrame mit Feature Importance und den Feature-Namen\n",
    "importance_df = pd.DataFrame({'importance': feature_importances, 'feature': features})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=True)\n",
    "\n",
    "# Visualisieren der Feature Importance in einem horizontalen Balkendiagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], align='center')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = 'sp.h5'\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    monthly_constituents_df = store.get('monthly_constituents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "    tickers_on_date = set()\n",
    "    for idx, row in monthly_constituents.iterrows():\n",
    "        if row['Date'] <= date:\n",
    "            tickers_on_date = row['Constituents']\n",
    "    return tickers_on_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "    tickers_on_date = set()\n",
    "    for idx, row in monthly_constituents.iterrows():\n",
    "        if row['Date'] <= pd.Timestamp(date):\n",
    "\n",
    "            tickers_on_date = row['Constituents']\n",
    "    return tickers_on_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "#     tickers_on_date = set()\n",
    "#     for idx, row in monthly_constituents.iterrows():\n",
    "#         if row['Date'] <= date:\n",
    "#             tickers_on_date = row['Constituents']\n",
    "#             #break\n",
    "#     return tickers_on_date\n",
    "\n",
    "\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     monthly_constituents = store.get('monatliche_bestandteile')\n",
    "\n",
    "# data_flat['in_sp500'] = [ticker in get_sp500_tickers_on_date(date, monthly_constituents) for date, ticker in zip(data_flat['date'], data_flat['ticker'])]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Backtest. Investition nur wenn Aktie zu dem Zeitpunkt Bestandteil des S&P500 war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant\n",
    "# Testen des besten Modells auf den Testdaten\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Drucken der Leistungsmetriken\n",
    "print(\"Testgenauigkeit: \", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Klassifikationsbericht Testdaten: \")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unknown_test_data['date']))\n",
    "print(len(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_signals(y_test_pred):\n",
    "    buy_signals = 0\n",
    "    sell_signals = 0\n",
    "\n",
    "    for signal in y_test_pred:\n",
    "        if signal == 1:\n",
    "            buy_signals += 1\n",
    "        elif signal == -1:\n",
    "            sell_signals += 1\n",
    "\n",
    "    return buy_signals, sell_signals\n",
    "\n",
    "buy_signals, sell_signals = count_signals(y_test_pred)\n",
    "\n",
    "print(f\"Anzahl der Kaufsignale: {buy_signals}\")\n",
    "print(f\"Anzahl der Verkaufssignale: {sell_signals}\")\n",
    "\n",
    "\n",
    "# Erstelle einen DataFrame mit Datum und Vorhersagen\n",
    "signal_data = pd.DataFrame({'date': unknown_test_data['date'], 'signal': y_test_pred})\n",
    "\n",
    "# Gruppiere Daten nach Datum und zähle Signale\n",
    "signal_counts = signal_data.groupby(['date', 'signal']).size().unstack(fill_value=0).reset_index()\n",
    "signal_counts.columns = ['date', 'sell', 'hold', 'buy']\n",
    "\n",
    "# Plotten der Signale im Zeitverlauf\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(signal_counts['date'], signal_counts['buy'], label='Kaufsignale')\n",
    "plt.plot(signal_counts['date'], signal_counts['sell'], label='Verkaufsignale')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Anzahl der Signale')\n",
    "plt.title('Anzahl der Kauf- und Verkaufssignale im Zeitverlauf')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_data = pd.DataFrame({'date': unknown_test_data['date'], 'signal': y_test_pred})\n",
    "# signal_data['sell'] = (signal_data['signal'] == -1).astype(int)\n",
    "# signal_data['buy'] = (signal_data['signal'] == 1).astype(int)\n",
    "\n",
    "# signal_counts = signal_data.groupby('date').agg({'sell': 'sum', 'buy': 'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtest\n",
    "initial_cash = 10000000\n",
    "max_stock_share = 0.05\n",
    "price_data = data_flat.pivot_table(values='adjusted_close', index='date', columns='ticker')\n",
    "\n",
    "portfolio = pd.DataFrame()\n",
    "portfolio['date'] = test_data['date'].unique()\n",
    "portfolio.set_index('date', inplace=True)\n",
    "portfolio['cash'] = 0\n",
    "portfolio['total_value'] = 0\n",
    "portfolio.sort_index(inplace=True)\n",
    "\n",
    "# Anfangswert von 'cash' und 'total_value' festzulegen\n",
    "first_date = portfolio.index[0]\n",
    "portfolio.at[first_date, 'cash'] = initial_cash\n",
    "portfolio.at[first_date, 'total_value'] = initial_cash\n",
    "\n",
    "unknown_test_data = unknown_test_data.sort_values(by=['date', 'ticker']).reset_index(drop=True)\n",
    "\n",
    "stock_positions = {}\n",
    "trades = pd.DataFrame(columns=['date', 'ticker', 'shares', 'action', 'price'])\n",
    "is_initial = True\n",
    "\n",
    "for idx, row in unknown_test_data.iterrows():\n",
    "    stock = row['ticker']\n",
    "    date = row['date']\n",
    "    signal = y_test_pred[idx]\n",
    "    cash = portfolio.loc[date, 'cash']\n",
    "    stock_value = row['adjusted_close']\n",
    "\n",
    "    if np.isnan(price_data.loc[date, stock]):\n",
    "        print(f\"Missing price data für {stock} an {date}\")\n",
    "\n",
    "    if is_initial:\n",
    "        portfolio.loc[date, 'cash'] = initial_cash\n",
    "        portfolio.loc[date, 'total_value'] = initial_cash\n",
    "        is_initial = False\n",
    "\n",
    "    elif signal == 1 and cash > stock_value * max_stock_share and stock in get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "\n",
    "        shares_to_buy = (cash * max_stock_share) // stock_value\n",
    "        cost = shares_to_buy * stock_value\n",
    "\n",
    "        portfolio.loc[date, 'cash'] -= cost\n",
    "        if stock not in stock_positions:\n",
    "            stock_positions[stock] = 0\n",
    "        stock_positions[stock] += shares_to_buy\n",
    "\n",
    "        trades = trades.append({'date': date, 'ticker': stock, 'shares': shares_to_buy, 'action': 'buy', 'price': stock_value}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    elif signal == -1 and stock in stock_positions and stock_positions[stock] > 0: #Alternativ auch bei Signal 0 verkaufen\n",
    "        shares_to_sell = stock_positions[stock]\n",
    "        revenue = shares_to_sell * stock_value\n",
    "\n",
    "        portfolio.loc[date, 'cash'] += revenue\n",
    "        stock_positions[stock] = 0\n",
    "\n",
    "        trades = trades.append({'date': date, 'ticker': stock, 'shares': shares_to_sell, 'action': 'sell', 'price': stock_value}, ignore_index=True)\n",
    "\n",
    "    total_stock_value = sum([stock_positions[ticker] * price_data.loc[date, ticker] for ticker in stock_positions if not np.isnan(price_data.loc[date, ticker])])\n",
    "    portfolio.loc[date, 'total_value'] = portfolio.loc[date, 'cash'] + total_stock_value\n",
    "\n",
    "    next_date = unknown_test_data.loc[idx + 1, 'date'] if idx + 1 < len(unknown_test_data) else None\n",
    "    if next_date is not None and next_date != date:\n",
    "        portfolio.loc[next_date, 'cash'] = portfolio.loc[date, 'cash']\n",
    "        portfolio.loc[next_date, 'total_value'] = portfolio.loc[date, 'total_value']\n",
    "\n",
    "start_date = test_data['date'].min().strftime('%Y-%m-%d')\n",
    "end_date = test_data['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "sp500_data = yf.download('^GSPC', start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "sp500_data = sp500_data.pct_change().dropna()\n",
    "sp500_data = (sp500_data + 1).cumprod() * initial_cash\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['total_value'], label='Portfolio')\n",
    "plt.plot(sp500_data.index, sp500_data, label='S&P 500')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Wert')\n",
    "plt.title('Portfolio Performance vs. S&P 500')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance in %\n",
    "# Gewinne in Prozent\n",
    "portfolio['percentage_gain'] = (portfolio['total_value'] / initial_cash - 1) * 100\n",
    "sp500_data_percentage = (sp500_data / initial_cash - 1) * 100\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['percentage_gain'], label='Portfolio')\n",
    "plt.plot(sp500_data.index, sp500_data_percentage, label='S&P 500')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Gewinn in %')\n",
    "plt.title('Portfolio Performance vs. S&P 500')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outperformance gegenüber S&P500 in Prozent berechnen\n",
    "portfolio['outperformance'] = portfolio['percentage_gain'] - sp500_data_percentage\n",
    "\n",
    "# Diagramm \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['outperformance'], label='Outperformance')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Outperformance in %')\n",
    "plt.title('Outperformance des Random Forest Portfolios gegenüber dem S&P 500')\n",
    "plt.legend()\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)  # Fügt eine horizontale Linie bei 0% hinzu\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "portfolio['weekly_returns'] = portfolio['total_value'].pct_change()\n",
    "sp500_daily_returns = sp500_data.pct_change()\n",
    "\n",
    "# Sharpe-Ratio berechnen\n",
    "risk_free_rate = 0.02  # Annahme eines risikofreien Zinssatzes von 2%\n",
    "portfolio_excess_returns = portfolio['weekly_returns'] - risk_free_rate / 252\n",
    "sp500_excess_returns = sp500_daily_returns - risk_free_rate / 252\n",
    "\n",
    "portfolio_sharpe_ratio = np.sqrt(252) * portfolio_excess_returns.mean() / portfolio_excess_returns.std()\n",
    "sp500_sharpe_ratio = np.sqrt(252) * sp500_excess_returns.mean() / sp500_excess_returns.std()\n",
    "\n",
    "# Diagramm\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Portfolio', 'S&P 500'], [portfolio_sharpe_ratio, sp500_sharpe_ratio])\n",
    "plt.xlabel('Investment')\n",
    "plt.ylabel('Sharpe-Ratio')\n",
    "plt.title('Sharpe-Ratio des Portfolios und des S&P 500')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backtest\n",
    "# initial_cash = 10000000\n",
    "# max_stock_share = 0.05\n",
    "# price_data = data_flat.pivot_table(values='adjusted_close', index='date', columns='ticker')\n",
    "\n",
    "# portfolio = pd.DataFrame()\n",
    "# portfolio['date'] = test_data['date'].unique()\n",
    "# portfolio.set_index('date', inplace=True)\n",
    "# portfolio['cash'] = 0\n",
    "# portfolio['total_value'] = 0\n",
    "# portfolio.sort_index(inplace=True)\n",
    "\n",
    "# # Anfangswert von 'cash' und 'total_value' festlegen\n",
    "# first_date = portfolio.index[0]\n",
    "# portfolio.at[first_date, 'cash'] = initial_cash\n",
    "# portfolio.at[first_date, 'total_value'] = initial_cash\n",
    "\n",
    "# unknown_test_data = unknown_test_data.sort_values(by=['date', 'ticker']).reset_index(drop=True)\n",
    "\n",
    "# stock_positions = {}\n",
    "# trades = pd.DataFrame(columns=['date', 'ticker', 'shares', 'action', 'price'])\n",
    "# is_initial = True\n",
    "\n",
    "# for idx, row in unknown_test_data.iterrows():\n",
    "#     stock = row['ticker']\n",
    "#     date = row['date']\n",
    "#     signal = y_test_pred[idx]\n",
    "#     cash = portfolio.loc[date, 'cash']\n",
    "#     stock_value = row['adjusted_close']\n",
    "\n",
    "#     if np.isnan(price_data.loc[date, stock]):\n",
    "#         print(f\"Missing price data für {stock} an {date}\")\n",
    "\n",
    "#     if is_initial:\n",
    "#         portfolio.loc[date, 'cash'] = initial_cash\n",
    "#         portfolio.loc[date, 'total_value'] = initial_cash\n",
    "#         is_initial = False\n",
    "\n",
    "#     if stock in get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "#         if signal == 1 and cash > stock_value * max_stock_share:\n",
    "#             shares_to_buy = (cash * max_stock_share) // stock_value\n",
    "#             cost = shares_to_buy * stock_value\n",
    "\n",
    "#             portfolio.loc[date, 'cash'] -= cost\n",
    "#             if stock not in stock_positions:\n",
    "#                 stock_positions[stock] = 0\n",
    "#             stock_positions[stock] += shares_to_buy\n",
    "\n",
    "#             new_trade = pd.DataFrame({'date': [date], 'ticker': [stock], 'shares': [shares_to_buy], 'action': ['buy'], 'price': [stock_value]})\n",
    "#             trades = pd.concat([trades, new_trade], ignore_index=True)\n",
    "\n",
    "#         elif signal == -1 and stock in stock_positions and stock_positions[stock] > 0:\n",
    "#             shares_to_sell = stock_positions[stock]\n",
    "#             revenue = shares_to_sell * stock_value\n",
    "\n",
    "#             portfolio.loc[date, 'cash'] += revenue\n",
    "#             stock_positions[stock] = 0\n",
    "\n",
    "#             new_trade = pd.DataFrame({'date': [date], 'ticker': [stock], 'shares': [shares_to_sell], 'action': ['sell'], 'price': [stock_value]})\n",
    "#             trades = pd.concat([trades, new_trade], ignore_index=True)\n",
    "            \n",
    "\n",
    "#         total_stock_value = sum([stock_positions[ticker] * price_data.loc[date, ticker] for ticker in stock_positions])\n",
    "#     portfolio.loc[date, 'total_value'] = portfolio.loc[date, 'cash'] + total_stock_value\n",
    "\n",
    "#     next_date = unknown_test_data.loc[idx + 1, 'date'] if idx + 1 < len(unknown_test_data) else None\n",
    "#     if next_date is not None and next_date != date:\n",
    "#         portfolio.loc[next_date, 'cash'] = portfolio.loc[date, 'cash']\n",
    "#         portfolio.loc[next_date, 'total_value'] = portfolio.loc[date, 'total_value']\n",
    "\n",
    "# start_date = test_data['date'].min().strftime('%Y-%m-%d')\n",
    "# end_date = test_data['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "# sp500_data = yf.download('^GSPC', start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "# sp500_data = sp500_data.pct_change().dropna()\n",
    "# sp500_data = (sp500_data + 1).cumprod() * initial_cash\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(portfolio.index, portfolio['total_value'], label='Portfolio')\n",
    "# plt.plot(sp500_data.index, sp500_data, label='S&P 500')\n",
    "# plt.xlabel('Datum')\n",
    "# plt.ylabel('Wert')\n",
    "# plt.title('Portfolio Performance vs. S&P 500')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Berechnung des Betas über den Zeitraum der Investition (gewichtet)\n",
    "# portfolio['weighted_beta'] = 0\n",
    "\n",
    "# # Änderung hier: Verwenden von unknown_test_data statt data_flat\n",
    "# beta_data = unknown_test_data.pivot_table(values='betas', index='date', columns='ticker')\n",
    "\n",
    "# # Filtern der Beta-Daten entsprechend dem Zeitraum des Backtests\n",
    "# beta_data = beta_data.loc[portfolio.index]\n",
    "\n",
    "# # Füge fehlende Beta-Daten als Spalte in unknown_test_data hinzu\n",
    "# unknown_test_data['beta'] = 0.0\n",
    "# for idx, row in unknown_test_data.iterrows():\n",
    "#     date = row['date']\n",
    "#     ticker = row['ticker']\n",
    "#     beta_value = beta_data.loc[date, ticker]\n",
    "#     if not np.isnan(beta_value):\n",
    "#         unknown_test_data.at[idx, 'beta'] = beta_value\n",
    "#     else:\n",
    "#         print(f\"Missing beta data for {ticker} on {date}\")\n",
    "\n",
    "# for date in portfolio.index:\n",
    "    \n",
    "#     positions_on_date = {ticker: shares for ticker, shares in stock_positions.items() if shares > 0}\n",
    "#     total_stock_value_on_date = sum([shares * price_data.loc[date, ticker] for ticker, shares in positions_on_date.items() if not np.isnan(price_data.loc[date, ticker])])\n",
    "\n",
    "#     if total_stock_value_on_date > 0:\n",
    "#         weighted_beta_on_date = sum([(shares * price_data.loc[date, ticker] / total_stock_value_on_date) * unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values[0] if not pd.isnull(unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values[0]) else 0 for ticker, shares in positions_on_date.items()])\n",
    "#     else:\n",
    "#         weighted_beta_on_date = 0\n",
    "\n",
    "#     portfolio.loc[date, 'weighted_beta'] = weighted_beta_on_date\n",
    "\n",
    "# # Plotten von Verlauf des gewichteten Betas während der Investition\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(portfolio.index, portfolio['weighted_beta'])\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Weighted Beta')\n",
    "# plt.title('Weighted Beta Over Time')\n",
    "# plt.show()\n",
    "\n",
    "# # Berechnen des gesamten gewichteten Beta über den gesamten Zeitraum\n",
    "# total_weighted_beta = portfolio['weighted_beta'].mean()\n",
    "# print(f\"Total Weighted Beta over the entire period: {total_weighted_beta:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noch testen verkauf auch bei 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Betas über den Zeitraum der Investition (gewichtet)\n",
    "portfolio['weighted_beta'] = 0\n",
    "\n",
    "# Änderung: Verwenden von unknown_test_data statt data_flat\n",
    "beta_data = unknown_test_data.pivot_table(values='betas', index='date', columns='ticker')\n",
    "\n",
    "# Filtern der Beta-Daten entsprechend dem Zeitraum des Backtests\n",
    "beta_data = beta_data.loc[portfolio.index]\n",
    "\n",
    "# Fügen der fehlende Beta-Daten als Spalte in unknown_test_data hinzu\n",
    "unknown_test_data['beta'] = 0.0\n",
    "for idx, row in unknown_test_data.iterrows():\n",
    "    date = row['date']\n",
    "    ticker = row['ticker']\n",
    "    beta_value = beta_data.loc[date, ticker]\n",
    "    if not np.isnan(beta_value):\n",
    "        unknown_test_data.at[idx, 'beta'] = beta_value\n",
    "    else:\n",
    "        print(f\"Missing beta data for {ticker} on {date}\")\n",
    "\n",
    "for date in portfolio.index:\n",
    "    \n",
    "    positions_on_date = {ticker: shares for ticker, shares in stock_positions.items() if shares > 0}\n",
    "    total_stock_value_on_date = sum([shares * price_data.loc[date, ticker] for ticker, shares in positions_on_date.items() if not np.isnan(price_data.loc[date, ticker])])\n",
    "\n",
    "    if total_stock_value_on_date > 0:\n",
    "        weighted_beta_on_date = sum([(shares * price_data.loc[date, ticker] / total_stock_value_on_date) * unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values[0] if len(unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values) > 0 else 0 for ticker, shares in positions_on_date.items()])\n",
    "    else:\n",
    "        weighted_beta_on_date = 0\n",
    "\n",
    "    portfolio.loc[date, 'weighted_beta'] = weighted_beta_on_date\n",
    "\n",
    "# Plotten von Verlauf des gewichteten Betas während der Investition\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['weighted_beta'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Gewichtetes Beta')\n",
    "plt.title('Gewichtetes Beta im Zeitverlauf')\n",
    "plt.show()\n",
    "\n",
    "# Berechnen des gesamten gewichteten Beta über den gesamten Zeitraum\n",
    "total_weighted_beta = portfolio['weighted_beta'].mean()\n",
    "print(f\"Gewichtetes Beta über gesamten Zeitraum der Investition: {total_weighted_beta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen Renditen des Portfolios und des S&P 500\n",
    "portfolio_returns = portfolio['total_value'].pct_change().dropna()\n",
    "sp500_returns = sp500_data.pct_change().dropna()\n",
    "\n",
    "# Berechnen durchschnittliche Renditen des Portfolios und des S&P 500\n",
    "average_portfolio_return = portfolio_returns.mean()\n",
    "average_sp500_return = sp500_returns.mean()\n",
    "\n",
    "# Alpha des Portfolios berechnen\n",
    "# Festlegen, der risikofreien Rendite\n",
    "risk_free_rate = 0.02\n",
    "alpha = average_portfolio_return - (risk_free_rate + total_weighted_beta * (average_sp500_return - risk_free_rate))\n",
    "\n",
    "print(f\"Alpha des Portfolios: {alpha:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
