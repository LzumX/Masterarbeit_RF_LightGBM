{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, BaseCrossValidator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "#Laden der Daten und Vorbereiten für das maschinelle Lernen:\n",
    "DATA_STORE = 'sp.h5'\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    data = store.get('data_clean')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        row.prices.date\n",
    "    except:\n",
    "        row.prices = row.prices.reset_index(level=['date'])\n",
    "\n",
    "data.rename_axis('ticker').reset_index()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    row.prices['ticker'] = index\n",
    "\n",
    "data_flat = pd.DataFrame(columns=['ticker', 'date', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'weekly_return', 'rsi', 'bb_low', 'bb_mid', 'bb_upper', 'target'])\n",
    "data_flat.set_index('ticker')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    data_flat = pd.concat([data_flat, row.prices])\n",
    "\n",
    "# Berechnung der wöchentlichen Renditen und Klassifizierung der Performance:\n",
    "data_flat['weekly_return'] = data_flat['adjusted_close'].pct_change(1).shift(-1)\n",
    "\n",
    "outperform_threshold = 0.015\n",
    "underperform_threshold = -0.01\n",
    "\n",
    "def classify_performance(weekly_return):\n",
    "    if weekly_return > outperform_threshold:\n",
    "        return 1\n",
    "    elif weekly_return < underperform_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data_flat['target'] = data_flat['weekly_return'].apply(classify_performance)\n",
    "data_flat = data_flat.dropna()\n",
    "\n",
    "data_flat['date'] = pd.to_datetime(data_flat['date'])\n",
    "data_flat = data_flat[data_flat['date'].dt.weekday == 4]\n",
    "\n",
    "#CPWR komplett entfernen\n",
    "data_flat = data_flat[data_flat['ticker'] != 'CPWR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative Walk Forward Testen:\n",
    "#Implementierung der Walk-Forward-Cross-Validation:\n",
    "class WalkForwardCV(BaseCrossValidator):\n",
    "    def __init__(self, n_splits=4, test_period_length=8, lookahead=1):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_period_length = test_period_length\n",
    "        self.lookahead = lookahead\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        for i in range(self.n_splits):\n",
    "            train_start = 0\n",
    "            train_end = i * self.test_period_length + (n_samples - self.test_period_length * (self.n_splits + self.lookahead - 1))\n",
    "            test_start = train_end\n",
    "            test_end = test_start + self.test_period_length\n",
    "            yield np.arange(train_start, train_end), np.arange(test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    monthly_constituents = store.get('monatliche_bestandteile')\n",
    "\n",
    "#Hinzufügen insp500 Der RF kann auf daten trainieren, die nicht im sp500 waren, aber anschließend nur in die zu dem Zeitpunkt enthaltenen Werte investieren\n",
    "def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "    tickers_on_date = set()\n",
    "    for idx, row in monthly_constituents.iterrows():\n",
    "        if row['Date'] <= date:\n",
    "            tickers_on_date = row['Constituents']\n",
    "            break\n",
    "    return tickers_on_date\n",
    "\n",
    "\n",
    "#data_flat['in_sp500'] = [ticker in get_sp500_tickers_on_date(date, monthly_constituents) for date, ticker in zip(data_flat['date'], data_flat['ticker'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'lgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'lgbm__n_estimators': [50, 100, 200],\n",
    "#     'lgbm__num_leaves': [16, 31, 64],\n",
    "#     'lgbm__max_depth': [-1, 8, 16],\n",
    "#     'lgbm__min_child_samples': [20, 50, 100],\n",
    "#     'lgbm__subsample': [0.6, 0.8, 1.0],\n",
    "#     'lgbm__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#     'lgbm__reg_alpha': [0.0, 0.1, 0.5],\n",
    "#     'lgbm__reg_lambda': [0.0, 0.1, 0.5]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung der Daten in Trainings-, Validierungs- und Testsets\n",
    "features = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'rsi', 'bb_low', 'bb_mid', 'bb_upper']\n",
    "target = 'target'\n",
    "\n",
    "# Anpassung der Zeitraum für Trainings- und Testphasen\n",
    "n_splits = 10\n",
    "train_period_length = 2  # 16 Wochen Trainingszeitraum\n",
    "test_period_length = 3    # 16 Wochen Testzeitraum\n",
    "lookahead = 1\n",
    "\n",
    "cv = WalkForwardCV(n_splits=n_splits, test_period_length=test_period_length, lookahead=lookahead)\n",
    "\n",
    "#Finanzkrise mit betrachten; Zeitraum ab 2010 definieren, da sich vielleicht Muster geändert haben\n",
    "\n",
    "\n",
    "train_start_date = '2009-12-31'\n",
    "train_end_date = '2010-12-31'\n",
    "valid_end_date = '2011-12-31'\n",
    "test_end_date = '2022-12-31'\n",
    "\n",
    "test_start_date = '2012-01-01'\n",
    "\n",
    "train_start_date = pd.Timestamp(train_start_date)\n",
    "train_end_date = pd.Timestamp(train_end_date)\n",
    "valid_end_date = pd.Timestamp(valid_end_date)\n",
    "test_end_date = pd.Timestamp(test_end_date)\n",
    "\n",
    "test_start_date = pd.Timestamp(test_start_date)\n",
    "\n",
    "\n",
    "train_data = data_flat[data_flat['date'] <= train_end_date]\n",
    "valid_data = data_flat[(data_flat['date'] > train_end_date) & (data_flat['date'] <= valid_end_date)]\n",
    "test_data = data_flat[(data_flat['date'] > valid_end_date) & (data_flat['date'] >= test_start_date) & (data_flat['date'] <= test_end_date)]\n",
    "\n",
    "X_train = train_data[features]\n",
    "\n",
    "\n",
    "y_train = train_data[target]\n",
    "X_valid = valid_data[features]\n",
    "\n",
    "\n",
    "y_valid = valid_data[target]\n",
    "X_test = test_data[features]\n",
    "\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Erweiterte Hyperparameter-Optimierung\n",
    "#class_weights = {-1: 1, 0: 1, 1: 1}  # Klasse 1 wird hier einfach gewichtet\n",
    "#class_weights = [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "class_weights = [None]# 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "pipeline = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='mean')),    \n",
    "    ('scaler', StandardScaler()),    \n",
    "    ('lgbm', lgb.LGBMClassifier(random_state=42))#, class_weight=class_weights))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lgbm__learning_rate': [0.01, 0.1],#, 0.1],\n",
    "    'lgbm__n_estimators': [50, 100, 200],#[50, 100]#, 200],\n",
    "    'lgbm__num_leaves': [16, 31, 64], #[16, 31],#, 64],    #[16, 31]\n",
    "    'lgbm__max_depth': [-1, 8, 16],#, 16],\n",
    "    'lgbm__min_child_samples': [20,50, 100],#,50, 100],\n",
    "    'lgbm__subsample': [0.6, 0.8],#, 1.0],\n",
    "    'lgbm__colsample_bytree': [0.6, 0.8],#, 1.0],\n",
    "    'lgbm__reg_alpha': [0.0, 0.5],#, 0.5],\n",
    "    'lgbm__reg_lambda': [0.0, 0.5],#, 0.5]\n",
    "    'lgbm__class_weight': class_weights\n",
    "}\n",
    "\n",
    "\n",
    "# Verwendung von GridSearchCV für Hyperparameter-Optimierung mit Genauigkeit als Scoring-Methode\n",
    "grid = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Alternative Scoring-Methode: F1-Score (Macro) statt Genauigkeit\n",
    "grid_f1 = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Messung der Laufzeit der Hyperparameter-Optimierung\n",
    "start_time = time.time()\n",
    "grid.fit(X_train, y_train)\n",
    "grid_f1.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Laufzeit der Hyperparameter-Optimierung: {:.2f} Minuten\".format((end_time - start_time) / 60))\n",
    "\n",
    "# Ergebnisse der Walk-Forward-Cross-Validation\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# Ergebnisse für jeden Split anzeigen\n",
    "for i in range(n_splits):\n",
    "    split_test_score = f'split{i}_test_score'\n",
    "    print(f\"Testergebnisse für Split {i + 1}:\")\n",
    "    print(cv_results[[split_test_score]].sort_values(by=split_test_score, ascending=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Ausgabe der besten Hyperparameter und Modell-Performance\n",
    "print(\"Beste Hyperparameter: \", grid.best_params_)\n",
    "print(\"Beste Modellgenauigkeit: {:.4f}\".format(grid.best_score_))\n",
    "print(\"Beste F1-Score (Macro) (Alternative): {:.4f}\".format(grid_f1.best_score_))\n",
    "\n",
    "# Testen des besten Modells auf den Validierungsdaten\n",
    "best_model = grid_f1.best_estimator_\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# Ausgabe der Leistungsmetricken\n",
    "print(\"Genauigkeit: \", accuracy_score(y_valid, y_valid_pred))\n",
    "print(\"Klassifikationsbericht: \")\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Testen des besten Modells auf den Testdaten\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Ausgabe der Leistungsmetriken für das Testset\n",
    "print(\"Test-Genauigkeit: \", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test-Klassifikationsbericht: \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Feature-Wichtigkeit\n",
    "feature_importance = best_model.named_steps['lgbm'].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(X_train.shape[1]), feature_importance[sorted_idx])\n",
    "plt.yticks(range(X_train.shape[1]), X_train.columns[sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance für das LightGBM Model')\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung der wöchentlichen Renditen und Vorhersagen\n",
    "test_data['predicted'] = y_test_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(test_data['date'], test_data['weekly_return'], label='True Returns')\n",
    "ax.scatter(test_data['date'][test_data['predicted'] == 1], test_data['weekly_return'][test_data['predicted'] == 1], color='g', label='Outperform Prediction')\n",
    "ax.scatter(test_data['date'][test_data['predicted'] == -1], test_data['weekly_return'][test_data['predicted'] == -1], color='r', label='Underperform Prediction')\n",
    "ax.set_xlabel('Datum')\n",
    "ax.set_ylabel('Weekly Return')\n",
    "ax.legend()\n",
    "plt.title('Weekly Returns und Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backtest auf Validierungs- und Testset\n",
    "\n",
    "validation_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "print(\"Validierungsgenauigkeit: {:.2f}%\".format(validation_accuracy * 100))\n",
    "\n",
    "print(\"\\nKlassifikationsbericht für Validierung:\")\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Erstellen von Maske, um den unbekannten Zeitraum im Testset zu extrahieren; eigtl. unnötig da testzeitraum definiert\n",
    "mask = test_data['date'] > valid_end_date # + pd.DateOffset(weeks=1)\n",
    "unknown_test_data = test_data[mask]\n",
    "unknown_X_test = unknown_test_data[features]\n",
    "unknown_y_test = unknown_test_data[target]\n",
    "\n",
    "# Vorhersagen auf dem unbekannten Zeitraum im Testset\n",
    "y_test_pred = grid.predict(unknown_X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(unknown_y_test, y_test_pred)\n",
    "print(\"Testgenauigkeit: {:.2f}%\".format(test_accuracy * 100))\n",
    "\n",
    "print(\"\\nKlassifikationsbericht für Test:\")\n",
    "print(classification_report(unknown_y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Der beste LGBM, der während der Hyperparameter-Optimierung gefunden wurde\n",
    "best_lgbm = grid.best_estimator_.named_steps['lgbm']\n",
    "\n",
    "# Berechnung der Feature Importance\n",
    "feature_importances = best_lgbm.feature_importances_\n",
    "\n",
    "# Erstellen eines DataFrame mit Feature Importance und den Feature-Namen\n",
    "importance_df = pd.DataFrame({'importance': feature_importances, 'feature': features})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=True)\n",
    "\n",
    "# Visualisieren der Feature Importance in einem horizontalen Balkendiagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], align='center')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Werte normalisiert für feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Der beste LGBM, der während der Hyperparameter-Optimierung gefunden wurde\n",
    "best_lgbm = grid.best_estimator_.named_steps['lgbm']\n",
    "\n",
    "# Berechnung der Feature Importance\n",
    "feature_importances = best_lgbm.feature_importances_\n",
    "\n",
    "# Normalisierung der Feature Importance, sodass sie sich zu 1 addieren\n",
    "normalized_importances = feature_importances / feature_importances.sum()\n",
    "\n",
    "# Erstellen eines DataFrame mit normalisierter Feature Importance und den Feature-Namen\n",
    "importance_df = pd.DataFrame({'importance': normalized_importances, 'feature': features})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=True)\n",
    "\n",
    "# Visualisieren der normalisierten Feature Importance in einem horizontalen Balkendiagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], align='center')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = 'sp.h5'\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    monthly_constituents_df = store.get('monthly_constituents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "    tickers_on_date = set()\n",
    "    for idx, row in monthly_constituents.iterrows():\n",
    "        if row['Date'] <= pd.Timestamp(date):\n",
    "\n",
    "            tickers_on_date = row['Constituents']\n",
    "    return tickers_on_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_signals(y_test_pred):\n",
    "    buy_signals = 0\n",
    "    sell_signals = 0\n",
    "\n",
    "    for signal in y_test_pred:\n",
    "        if signal == 1:\n",
    "            buy_signals += 1\n",
    "        elif signal == -1:\n",
    "            sell_signals += 1\n",
    "\n",
    "    return buy_signals, sell_signals\n",
    "\n",
    "buy_signals, sell_signals = count_signals(y_test_pred)\n",
    "\n",
    "print(f\"Anzahl der Kaufsignale: {buy_signals}\")\n",
    "print(f\"Anzahl der Verkaufssignale: {sell_signals}\")\n",
    "\n",
    "\n",
    "# Erstelle einen DataFrame mit Datum und Vorhersagen\n",
    "signal_data = pd.DataFrame({'date': unknown_test_data['date'], 'signal': y_test_pred})\n",
    "\n",
    "# Gruppiere Daten nach Datum und zähle Signale\n",
    "signal_counts = signal_data.groupby(['date', 'signal']).size().unstack(fill_value=0).reset_index()\n",
    "signal_counts.columns = ['date', 'sell', 'hold', 'buy']\n",
    "\n",
    "# Plotten der Signale im Zeitverlauf\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(signal_counts['date'], signal_counts['buy'], label='Kaufsignale')\n",
    "plt.plot(signal_counts['date'], signal_counts['sell'], label='Verkaufsignale')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Anzahl der Signale')\n",
    "plt.title('Anzahl der Kauf- und Verkaufssignale im Zeitverlauf')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtest\n",
    "initial_cash = 10000000\n",
    "max_stock_share = 0.05\n",
    "price_data = data_flat.pivot_table(values='adjusted_close', index='date', columns='ticker')\n",
    "\n",
    "portfolio = pd.DataFrame()\n",
    "portfolio['date'] = test_data['date'].unique()\n",
    "portfolio.set_index('date', inplace=True)\n",
    "portfolio['cash'] = 0\n",
    "portfolio['total_value'] = 0\n",
    "portfolio.sort_index(inplace=True)\n",
    "\n",
    "# Anfangswert von 'cash' und 'total_value' festlegen\n",
    "first_date = portfolio.index[0]\n",
    "portfolio.at[first_date, 'cash'] = initial_cash\n",
    "portfolio.at[first_date, 'total_value'] = initial_cash\n",
    "\n",
    "unknown_test_data = unknown_test_data.sort_values(by=['date', 'ticker']).reset_index(drop=True)\n",
    "\n",
    "stock_positions = {}\n",
    "trades = pd.DataFrame(columns=['date', 'ticker', 'shares', 'action', 'price'])\n",
    "is_initial = True\n",
    "\n",
    "for idx, row in unknown_test_data.iterrows():\n",
    "    stock = row['ticker']\n",
    "    date = row['date']\n",
    "    signal = y_test_pred[idx]\n",
    "    cash = portfolio.loc[date, 'cash']\n",
    "    stock_value = row['adjusted_close']\n",
    "\n",
    "    if np.isnan(price_data.loc[date, stock]):\n",
    "        print(f\"Missing price data für {stock} an {date}\")\n",
    "\n",
    "    if is_initial:\n",
    "        portfolio.loc[date, 'cash'] = initial_cash\n",
    "        portfolio.loc[date, 'total_value'] = initial_cash\n",
    "        is_initial = False\n",
    "\n",
    "    elif signal == 1 and cash > stock_value * max_stock_share and stock in get_sp500_tickers_on_date(date, monthly_constituents):\n",
    "\n",
    "        shares_to_buy = (cash * max_stock_share) // stock_value\n",
    "        cost = shares_to_buy * stock_value\n",
    "\n",
    "        portfolio.loc[date, 'cash'] -= cost\n",
    "        if stock not in stock_positions:\n",
    "            stock_positions[stock] = 0\n",
    "        stock_positions[stock] += shares_to_buy\n",
    "\n",
    "        trades = trades.append({'date': date, 'ticker': stock, 'shares': shares_to_buy, 'action': 'buy', 'price': stock_value}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    elif signal == -1 and stock in stock_positions and stock_positions[stock] > 0: #Alternativ auch bei Signal 0 verkaufen\n",
    "        shares_to_sell = stock_positions[stock]\n",
    "        revenue = shares_to_sell * stock_value\n",
    "\n",
    "        portfolio.loc[date, 'cash'] += revenue\n",
    "        stock_positions[stock] = 0\n",
    "\n",
    "        trades = trades.append({'date': date, 'ticker': stock, 'shares': shares_to_sell, 'action': 'sell', 'price': stock_value}, ignore_index=True)\n",
    "\n",
    "    total_stock_value = sum([stock_positions[ticker] * price_data.loc[date, ticker] for ticker in stock_positions if not np.isnan(price_data.loc[date, ticker])])\n",
    "    portfolio.loc[date, 'total_value'] = portfolio.loc[date, 'cash'] + total_stock_value\n",
    "\n",
    "    next_date = unknown_test_data.loc[idx + 1, 'date'] if idx + 1 < len(unknown_test_data) else None\n",
    "    if next_date is not None and next_date != date:\n",
    "        portfolio.loc[next_date, 'cash'] = portfolio.loc[date, 'cash']\n",
    "        portfolio.loc[next_date, 'total_value'] = portfolio.loc[date, 'total_value']\n",
    "\n",
    "start_date = test_data['date'].min().strftime('%Y-%m-%d')\n",
    "end_date = test_data['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "sp500_data = yf.download('^GSPC', start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "sp500_data = sp500_data.pct_change().dropna()\n",
    "sp500_data = (sp500_data + 1).cumprod() * initial_cash\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['total_value'], label='Portfolio')\n",
    "plt.plot(sp500_data.index, sp500_data, label='S&P 500')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Wert')\n",
    "plt.title('Portfolio Performance vs. S&P 500')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance in %\n",
    "# Gewinne in Prozent\n",
    "portfolio['percentage_gain'] = (portfolio['total_value'] / initial_cash - 1) * 100\n",
    "sp500_data_percentage = (sp500_data / initial_cash - 1) * 100\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['percentage_gain'], label='Portfolio')\n",
    "plt.plot(sp500_data.index, sp500_data_percentage, label='S&P 500')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Gewinn in %')\n",
    "plt.title('Portfolio Performance vs. S&P 500')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outperformance gegenüber S&P500 in Prozent berechnen\n",
    "portfolio['outperformance'] = portfolio['percentage_gain'] - sp500_data_percentage\n",
    "\n",
    "# Diagramm \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['outperformance'], label='Outperformance')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Outperformance in %')\n",
    "plt.title('Outperformance des LightGBM Portfolios gegenüber dem S&P 500')\n",
    "plt.legend()\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)  # Fügt eine horizontale Linie bei 0% hinzu\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "portfolio['weekly_returns'] = portfolio['total_value'].pct_change()\n",
    "sp500_daily_returns = sp500_data.pct_change()\n",
    "\n",
    "# Sharpe-Ratio berechnen\n",
    "risk_free_rate = 0.02  # Annahme eines risikofreien Zinssatzes von 2%\n",
    "portfolio_excess_returns = portfolio['weekly_returns'] - risk_free_rate / 252\n",
    "sp500_excess_returns = sp500_daily_returns - risk_free_rate / 252\n",
    "\n",
    "portfolio_sharpe_ratio = np.sqrt(252) * portfolio_excess_returns.mean() / portfolio_excess_returns.std()\n",
    "sp500_sharpe_ratio = np.sqrt(252) * sp500_excess_returns.mean() / sp500_excess_returns.std()\n",
    "\n",
    "# Diagramm \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Portfolio', 'S&P 500'], [portfolio_sharpe_ratio, sp500_sharpe_ratio])\n",
    "plt.xlabel('Investment')\n",
    "plt.ylabel('Sharpe-Ratio')\n",
    "plt.title('Sharpe-Ratio des Portfolios und des S&P 500')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Betas über den Zeitraum der Investition (gewichtet)\n",
    "portfolio['weighted_beta'] = 0\n",
    "\n",
    "# Änderung: Verwenden von unknown_test_data statt data_flat\n",
    "beta_data = unknown_test_data.pivot_table(values='betas', index='date', columns='ticker')\n",
    "\n",
    "# Filtern der Beta-Daten entsprechend dem Zeitraum des Backtests\n",
    "beta_data = beta_data.loc[portfolio.index]\n",
    "\n",
    "# Fehlende Beta-Daten als Spalte in unknown_test_data hinzufügen\n",
    "unknown_test_data['beta'] = 0.0\n",
    "for idx, row in unknown_test_data.iterrows():\n",
    "    date = row['date']\n",
    "    ticker = row['ticker']\n",
    "    beta_value = beta_data.loc[date, ticker]\n",
    "    if not np.isnan(beta_value):\n",
    "        unknown_test_data.at[idx, 'beta'] = beta_value\n",
    "    else:\n",
    "        print(f\"Missing beta data for {ticker} on {date}\")\n",
    "\n",
    "for date in portfolio.index:\n",
    "    \n",
    "    positions_on_date = {ticker: shares for ticker, shares in stock_positions.items() if shares > 0}\n",
    "    total_stock_value_on_date = sum([shares * price_data.loc[date, ticker] for ticker, shares in positions_on_date.items() if not np.isnan(price_data.loc[date, ticker])])\n",
    "\n",
    "    if total_stock_value_on_date > 0:\n",
    "        weighted_beta_on_date = sum([(shares * price_data.loc[date, ticker] / total_stock_value_on_date) * unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values[0] if len(unknown_test_data.loc[(unknown_test_data['date'] == date) & (unknown_test_data['ticker'] == ticker), 'beta'].values) > 0 else 0 for ticker, shares in positions_on_date.items()])\n",
    "    else:\n",
    "        weighted_beta_on_date = 0\n",
    "\n",
    "    portfolio.loc[date, 'weighted_beta'] = weighted_beta_on_date\n",
    "\n",
    "# Plotten von Verlauf des gewichteten Betas während der Investition\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio.index, portfolio['weighted_beta'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Gewichtetes Beta')\n",
    "plt.title('Gewichtetes Beta im Zeitverlauf')\n",
    "plt.show()\n",
    "\n",
    "# Berechnen des gesamten gewichteten Beta über den gesamten Zeitraum\n",
    "total_weighted_beta = portfolio['weighted_beta'].mean()\n",
    "print(f\"Gewichtetes Beta über gesamten Zeitraum der Investition: {total_weighted_beta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen Renditen des Portfolios und des S&P 500\n",
    "portfolio_returns = portfolio['total_value'].pct_change().dropna()\n",
    "sp500_returns = sp500_data.pct_change().dropna()\n",
    "\n",
    "# Berechnen durchschnittliche Renditen des Portfolios und des S&P 500\n",
    "average_portfolio_return = portfolio_returns.mean()\n",
    "average_sp500_return = sp500_returns.mean()\n",
    "\n",
    "# Alpha des Portfolios berechnen\n",
    "# Festlegen, der risikofreien Rendite\n",
    "risk_free_rate = 0.02\n",
    "alpha = average_portfolio_return - (risk_free_rate + total_weighted_beta * (average_sp500_return - risk_free_rate))\n",
    "\n",
    "print(f\"Alpha des Portfolios: {alpha:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
